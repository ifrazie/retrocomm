# Requirements Document

## Introduction

This specification addresses the issue of unwanted special tokens (specifically `<|*.*|>` patterns) appearing in the LLM-generated responses displayed in the pager interface. These tokens are internal model markers that should be filtered out before displaying to users.

## Glossary

- **LLM**: Large Language Model - The AI system generating chatbot responses
- **Special Tokens**: Internal markers used by language models (e.g., `<|start|>`, `<|end|>`, `<|channel|>`, `<|message|>`)
- **Response Content**: The text generated by the LLM that is displayed to users
- **LLMChatbotService**: The singleton service managing LLM interactions
- **Pager Interface**: The retro-styled message display component

## Requirements

### Requirement 1

**User Story:** As a user viewing messages in the pager interface, I want to see clean, readable responses without technical artifacts, so that the retro messaging experience feels authentic and professional.

#### Acceptance Criteria

1. WHEN the LLM generates a response containing special tokens matching the pattern `<|*|>`, THE LLMChatbotService SHALL remove all such tokens from the response content before returning it
2. WHEN special tokens are removed, THE LLMChatbotService SHALL preserve all other text content including whitespace and formatting
3. WHEN the response contains multiple special token patterns, THE LLMChatbotService SHALL remove all occurrences
4. WHEN the response contains no special tokens, THE LLMChatbotService SHALL return the content unchanged
5. THE LLMChatbotService SHALL apply token filtering to both streaming and non-streaming responses

### Requirement 2

**User Story:** As a developer maintaining the codebase, I want a reusable utility function for cleaning LLM responses, so that token filtering can be consistently applied across the application.

#### Acceptance Criteria

1. THE System SHALL provide a utility function named `cleanLLMResponse` in the utils directory
2. THE `cleanLLMResponse` function SHALL accept a string parameter containing the raw LLM response
3. THE `cleanLLMResponse` function SHALL return a string with all special token patterns removed
4. THE `cleanLLMResponse` function SHALL use a regular expression to match patterns like `<|text|>`, `<|start|>`, `<|end|>`, etc.
5. THE `cleanLLMResponse` function SHALL be exported for use in other modules

### Requirement 3

**User Story:** As a quality assurance tester, I want automated tests verifying token removal, so that I can ensure the filtering works correctly across different scenarios.

#### Acceptance Criteria

1. THE System SHALL include unit tests for the `cleanLLMResponse` utility function
2. THE tests SHALL verify removal of single special tokens
3. THE tests SHALL verify removal of multiple special tokens in one response
4. THE tests SHALL verify preservation of normal text content
5. THE tests SHALL verify handling of edge cases like empty strings and strings without tokens
